{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422b539d-0e61-48c2-9546-ff3c2d83df95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18483c0-85ba-4c5a-8161-4e1ee62d09c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = beam.pipeline() #creation of pipeline object\n",
    "\n",
    "\n",
    "#creation of pcoolection that is created after execution of transformation\n",
    "attendace_count(\n",
    "    p1\n",
    "     |beam.io.ReadFromText('sample_data/dept_data.txt')\n",
    ")\n",
    "\n",
    "\n",
    "## Types of input transforms (input source)\n",
    "# ReadFromText\n",
    "## file_pattern (can contain *.txt to read all the files in one go)\n",
    "## min_bundles_size -> batching while reading ?\n",
    "## compression_type\n",
    "## strip_trailing_newlines\n",
    "## validate : verify presence of file while pipeline gets created \n",
    "## skip_header_lines : ignore headers\n",
    "\n",
    "#ReadFromAvro\n",
    "#ReadFromParquet\n",
    "#ReadFromTFRecord (TFRecord : faster for network transmissions)\n",
    "#ReadFromMessagingQueues (ReadFromPubSub)\n",
    "\n",
    "\n",
    "## Types of output transforms \n",
    "# WriteToText --> num_of_shards\n",
    "# WriteToAvro\n",
    "# WriteToParquet --> row_group_buffer_size & record_batch_size\n",
    "# WriteToTFRecords\n",
    "# WriteToPubSub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a258eda2-2117-4a2f-9c84-e418872ccb7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample writing output\n",
    "\n",
    "p2 = beam.Pipeline()\n",
    "\n",
    "# lines = (\n",
    "#     p2\n",
    "#     | beam.Create(['line1','line2','line3','line3'])\n",
    "#     | beam.io.WriteToText('sample_data/outputtest')\n",
    "# )\n",
    "\n",
    "#KVP\n",
    "\n",
    "lines3 = (\n",
    "    p2\n",
    "    | beam.Create({'k1':{'name':'me','age':12}})\n",
    "    # | beam.Map(lambda element:element)\n",
    "    | beam.io.WriteToText('sample_data/outputtest_kvp')\n",
    "    \n",
    ")\n",
    "\n",
    "p2.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38319b9-35bf-418c-a334-a03895747ce2",
   "metadata": {},
   "source": [
    "## Transforms : Map, FlatMap, Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4b1846-d663-429e-9d3c-3fd48241a2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def SplitRow(element):\n",
    "    return element.split(',')\n",
    "\n",
    "def Filter(record):\n",
    "    return record[3] == 'Accounts'\n",
    "    \n",
    "    \n",
    "p2 = beam.Pipeline()\n",
    "\n",
    "\n",
    "\n",
    "_ = (\n",
    "\n",
    "    p2\n",
    "    | beam.io.ReadFromText('sample_data/dept_data.txt')\n",
    "    | beam.Map(SplitRow) #takes one element and outputs one element [one to one transformation]\n",
    "    | beam.Filter(Filter)\n",
    "    | beam.Map(lambda record : (record[1],1)) #emits kvp\n",
    "    | beam.CombinePerKey(sum) # groupby , combiner + reducer\n",
    "    | beam.io.WriteToText('sample_output/test')\n",
    ")\n",
    "\n",
    "p2.run()\n",
    "\n",
    "# flatMap -> emit multiple elements for a single element \n",
    "# in the above example Map creates an array of 5 elements, but flatmap would create 5 unique elements\n",
    "\n",
    "# Lambda function \n",
    "# result = lambda x,y : x*y --> x,y are arguments and x*y is expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ea95a2-fd51-4a8f-983a-e244fe5d4348",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 20 sample_output/test-00000-of-00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1317507-3e88-4245-af8c-5829d30c9f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAP - FLAT MAP - FILTER -- PART 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f31083-2334-473e-b318-01edf5965604",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def SplitRow(element):\n",
    "    return element.split(',')\n",
    "\n",
    "def Filter(record):\n",
    "    return record[3] == 'Accounts'\n",
    "    \n",
    "    \n",
    "#p2 = beam.Pipeline()\n",
    "\n",
    "with beam.Pipeline() as p1:\n",
    "\n",
    "  _ = (\n",
    "\n",
    "    p1\n",
    "    | 'Read from file' >> beam.io.ReadFromText('sample_data/dept_data.txt')\n",
    "    | 'Splitting based on ,' >>beam.Map(SplitRow) #takes one element and outputs one element [one to one transformation]\n",
    "    | 'Filter based on accoutns' >> beam.Filter(Filter)\n",
    "    | beam.Map(lambda record : (record[1],1)) #emits kvp\n",
    "    | beam.CombinePerKey(sum) # groupby , combiner + reducer\n",
    "    | beam.io.WriteToText('sample_output/test')\n",
    "  )\n",
    "\n",
    "#p2.run()\n",
    "\n",
    "# flatMap -> emit multiple elements for a single element \n",
    "# in the above example Map creates an array of 5 elements, but flatmap would create 5 unique elements\n",
    "\n",
    "# Lambda function \n",
    "# result = lambda x,y : x*y --> x,y are arguments and x*y is expression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6732d40-1192-4d48-ad91-fe6b1a28f4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# branching pipelines \n",
    "# simple exmaple would be two types of filter leading to two different types of ops\n",
    "# post branching the pipelines run in parallel independently "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95202c43-e2b1-4c37-8ec5-55f1d5fa141a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SplitRow(element):\n",
    "    return element.split(',')\n",
    "\n",
    "\n",
    "with beam.Pipeline() as p1:\n",
    "    input_collection = (\n",
    "                            p1\n",
    "                            |\"Read from file\" >> beam.io.ReadFromText('sample_data/dept_data.txt')\n",
    "                            |\"split_rows\" >> beam.Map(SplitRow)\n",
    "        \n",
    "                        )\n",
    "    \n",
    "    accounts_filter = (\n",
    "        input_collection\n",
    "        | 'Get all dept of accounts' >> beam.Filter(lambda record: record[3] == 'Accounts')\n",
    "        | 'Set record 1'     >> beam.Map(lambda record : (record[1],1))\n",
    "        | 'Group and sum' >> beam.CombinePerKey(sum)\n",
    "        | 'Write' >> beam.io.WriteToText('sample_output/accounts')\n",
    "    )\n",
    "    \n",
    "    hr_filter = (\n",
    "        input_collection\n",
    "        | 'Get all dept of hr' >> beam.Filter(lambda record: record[3] == 'HR')\n",
    "        | 'hr Set record 1'     >> beam.Map(lambda record : (\"HR, \" + record[1],1))\n",
    "        | 'hr Group and sum' >> beam.CombinePerKey(sum)\n",
    "        | 'hr Write' >> beam.io.WriteToText('sample_output/hr')\n",
    "\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d52bed-b22e-4d26-98c5-fbf36ff86499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten : Union operation (combines P collections\n",
    "# sample data type and cols required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f18972-cd74-4072-812d-3fa73cc84e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SplitRow(element):\n",
    "    return element.split(',')\n",
    "\n",
    "\n",
    "with beam.Pipeline() as p1:\n",
    "    input_collection = (\n",
    "                            p1\n",
    "                            |\"Read from file\" >> beam.io.ReadFromText('sample_data/dept_data.txt')\n",
    "                            |\"split_rows\" >> beam.Map(SplitRow)\n",
    "        \n",
    "                        )\n",
    "    \n",
    "    accounts_filter = (\n",
    "        input_collection\n",
    "        | 'Get all dept of accounts' >> beam.Filter(lambda record: record[3] == 'Accounts')\n",
    "        | 'Set record 1'     >> beam.Map(lambda record : ('accounts' + record[1],1))\n",
    "        | 'Group and sum' >> beam.CombinePerKey(sum)\n",
    "        #| 'Write' >> beam.io.WriteToText('sample_output/accounts')\n",
    "    )\n",
    "    \n",
    "    hr_filter = (\n",
    "        input_collection\n",
    "        | 'Get all dept of hr' >> beam.Filter(lambda record: record[3] == 'HR')\n",
    "        | 'hr Set record 1'     >> beam.Map(lambda record : ('HR' + record[1],1))\n",
    "        | 'hr Group and sum' >> beam.CombinePerKey(sum)\n",
    "        #| 'hr Write' >> beam.io.WriteToText('sample_output/hr')\n",
    "\n",
    "    )\n",
    "    \n",
    "    output = (\n",
    "        (accounts_filter,hr_filter)\n",
    "        |beam.Flatten()\n",
    "        |beam.io.WriteToText('sample_output/flatten')\n",
    "        \n",
    "    \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d88fa8e-5e82-41ff-9d11-50d0a36b38cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARDO Transformation\n",
    "# parallel processing\n",
    "# takes element -> proccesses -> return \n",
    "# parent calss for map and flatmap\n",
    "# find out more on pardo \n",
    "# contains dofn ... for distributed compute implementation .. create a class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d0c025-0c2c-44d8-9629-2867e6f1e96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SplitRow(beam.DoFn): #inherits DoFn\n",
    "    def process(self,element):\n",
    "        return [element.split(',')]\n",
    "    \n",
    "\n",
    "    \n",
    "with beam.Pipeline() as p1:\n",
    "    input_collection = (\n",
    "                            p1\n",
    "                            |\"Read from file\" >> beam.io.ReadFromText('sample_data/dept_data.txt')\n",
    "                            |\"split_rows\" >> beam.ParDo(SplitRow())\n",
    "                            |beam.io.WriteToText('sample_output/pardotest')\n",
    "        \n",
    "                        )\n",
    "#  filter and other transformation can be specified in pardo \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1325f1ee-f50f-4334-92c4-653296426138",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using pardo and grouping\n",
    "\n",
    "def SplitRow(element):\n",
    "    return element.split(',')\n",
    "\n",
    "\n",
    "class GroupSum(beam.DoFn):\n",
    "    def process(self,element):\n",
    "        (key,value) = element\n",
    "        return [(key,sum(value))]\n",
    "\n",
    "# class grouping(beam.DoFn):\n",
    "#     def process(self,element):\n",
    "        \n",
    "with beam.Pipeline() as p1:\n",
    "    input_collection = (\n",
    "                            p1\n",
    "                            |\"Read from file\" >> beam.io.ReadFromText('sample_data/dept_data.txt')\n",
    "                            |\"split_rows\" >> beam.Map(SplitRow)\n",
    "        \n",
    "                        )\n",
    "    \n",
    "    accounts_filter = (\n",
    "        input_collection\n",
    "        | 'Get all dept of accounts' >> beam.Filter(lambda record: record[3] == 'Accounts')\n",
    "        | 'Set record 1'     >> beam.Map(lambda record : (record[1],1))\n",
    "        | 'group' >> beam.GroupByKey()\n",
    "        | 'Group sum' >> beam.ParDo(GroupSum())\n",
    "        | 'Write' >> beam.io.WriteToText('sample_output/ltest1')\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
